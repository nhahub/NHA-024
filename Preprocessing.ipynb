{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Detection and Cropping Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates an automated pipeline for detecting and cropping hand regions from images using **MediaPipe**'s hand detection model.\n",
    "\n",
    "### Key Features:\n",
    "- Robust hand detection using MediaPipe\n",
    "- Automatic cropping with intelligent padding\n",
    "- Visual output for quality verification\n",
    "- Batch processing capability\n",
    "\n",
    "### Use Cases:\n",
    "- Preparing datasets for gesture recognition\n",
    "- Sign language processing\n",
    "- Hand pose estimation preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll use the following libraries:\n",
    "- **OpenCV (cv2)**: Image processing and I/O\n",
    "- **MediaPipe**: Google's hand detection solution\n",
    "- **NumPy**: Numerical operations\n",
    "- **Matplotlib**: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize MediaPipe Hand Detection\n",
    "\n",
    "### Configuration Parameters:\n",
    "- **static_image_mode=True**: Optimized for processing static images (not video streams)\n",
    "- **max_num_hands=1**: Detect only one hand per image\n",
    "- **min_detection_confidence=0.5**: Minimum confidence threshold (50%)\n",
    "\n",
    "### How it works:\n",
    "MediaPipe detects 21 hand landmarks including:\n",
    "- Wrist (1 point)\n",
    "- Fingers: Thumb, Index, Middle, Ring, Pinky (4 points each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hands solution\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Configure hand detection parameters\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=True,      # Process each image independently\n",
    "    max_num_hands=1,              # Detect only one hand\n",
    "    min_detection_confidence=0.5  # 50% confidence threshold\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Hand Cropping Function\n",
    "\n",
    "### Algorithm Steps:\n",
    "1. **Convert color space**: BGR → RGB (MediaPipe requirement)\n",
    "2. **Detect hand landmarks**: Process image through MediaPipe\n",
    "3. **Calculate bounding box**: Find min/max coordinates from all 21 landmarks\n",
    "4. **Add padding**: 20 pixels on all sides for better context\n",
    "5. **Crop image**: Extract the hand region\n",
    "\n",
    "### Returns:\n",
    "- Cropped image if hand detected\n",
    "- `None` if no hand found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_hand(image):\n",
    "    \"\"\"\n",
    "    Detects the hand and returns the cropped region.\n",
    "    Returns: cropped_image OR None if no hand detected.\n",
    "    \"\"\"\n",
    "\n",
    "    h, w, _ = image.shape\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb)\n",
    "\n",
    "    if not results.multi_hand_landmarks:\n",
    "        return None\n",
    "\n",
    "    # Get bounding box from landmarks\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    for lm in results.multi_hand_landmarks[0].landmark:\n",
    "        x_coords.append(int(lm.x * w))\n",
    "        y_coords.append(int(lm.y * h))\n",
    "\n",
    "    x_min, x_max = max(min(x_coords) - 20, 0), min(max(x_coords) + 20, w)\n",
    "    y_min, y_max = max(min(y_coords) - 20, 0), min(max(y_coords) + 20, h)\n",
    "\n",
    "    cropped = image[y_min:y_max, x_min:x_max]\n",
    "    return cropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the Pipeline\n",
    "\n",
    "Let's test our hand detection and cropping pipeline on a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = r\"D:\\Natiq project proposal\\preprocessing\\clean_dataset\\0032\\02_02_0032_(15_11_16_17_42_57)_c_0001.jpg\"  # change this\n",
    "img = cv2.imread(test_path)\n",
    "cropped = crop_hand(img)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Original\")\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Cropped Hand\")\n",
    "if cropped is not None:\n",
    "    plt.imshow(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing Example\n",
    "\n",
    "Process multiple images from a directory and save the cropped results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "input_dataset = r\"D:\\Natiq project proposal\\preprocessing\\dataset\"\n",
    "output_dataset = r\"D:\\Work\\Depi\\Technical\\Natiq\\project\\clean_dataset\"\n",
    "resize_shape = (224, 224)\n",
    "\n",
    "os.makedirs(output_dataset, exist_ok=True)\n",
    "\n",
    "def crop_and_save(img_path, save_folder):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return False\n",
    "    cropped = crop_hand(img)\n",
    "    if cropped is None:\n",
    "        return False\n",
    "    resized = cv2.resize(cropped, resize_shape)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    save_path = os.path.join(save_folder, os.path.basename(img_path))\n",
    "    cv2.imwrite(save_path, resized)\n",
    "    return True\n",
    "\n",
    "images_saved = 0\n",
    "\n",
    "for root, dirs, files in os.walk(input_dataset):\n",
    "    # create a corresponding folder in clean_dataset\n",
    "    relative_path = os.path.relpath(root, input_dataset)\n",
    "    save_folder = os.path.join(output_dataset, relative_path)\n",
    "\n",
    "    for f in files:\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            img_path = os.path.join(root, f)\n",
    "            if crop_and_save(img_path, save_folder):\n",
    "                images_saved += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleanup\n",
    "\n",
    "Release MediaPipe resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close MediaPipe Hands\n",
    "hands.close()\n",
    "print(\"✓ MediaPipe resources released\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This preprocessing pipeline successfully:\n",
    "- Detects hands in images using MediaPipe\n",
    "- Crops hand regions with appropriate padding\n",
    "- Processes images in batch mode\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
